{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MLSA Transformer Project: Python Code Summarization (OPTIMIZED V2)\n",
                "\n",
                "**CRITICAL**: If you see an `OutOfMemoryError`, please go to the top menu and select **Runtime -> Restart session**, then run all cells again. This clears the GPU memory.\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q torch datasets transformers torchmetrics tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from datasets import load_dataset\n",
                "from transformers import RobertaTokenizer\n",
                "import numpy as np\n",
                "import copy\n",
                "import math\n",
                "import os\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Optimized Dataset Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CodeSummarizationDataset(Dataset):\n",
                "    def __init__(self, split, tokenizer, max_code_len=256, max_summary_len=128, subset_size=None):\n",
                "        self.dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\", split=split)\n",
                "        if subset_size and subset_size < len(self.dataset):\n",
                "            self.dataset = self.dataset.select(range(subset_size))\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_code_len = max_code_len\n",
                "        self.max_summary_len = max_summary_len\n",
                "    def __len__(self): return len(self.dataset)\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.dataset[idx]\n",
                "        c_enc = self.tokenizer(item['code'], max_length=self.max_code_len, padding='max_length', truncation=True, return_tensors='pt')\n",
                "        s_enc = self.tokenizer(item['docstring'], max_length=self.max_summary_len, padding='max_length', truncation=True, return_tensors='pt')\n",
                "        return {'input_ids': c_enc['input_ids'].squeeze(0), 'labels': s_enc['input_ids'].squeeze(0)}\n",
                "\n",
                "def get_dataloaders(batch_size=64):\n",
                "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
                "    train_ds = CodeSummarizationDataset('train', tokenizer, subset_size=50000)\n",
                "    val_ds = CodeSummarizationDataset('validation', tokenizer, subset_size=5000)\n",
                "    print(f\"Dataset Statistics: Train={len(train_ds)}, Val={len(val_ds)}\")\n",
                "    return DataLoader(train_ds, batch_size=batch_size, shuffle=True), DataLoader(val_ds, batch_size=batch_size), tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transformer Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, max_len, d_model):\n",
                "        super().__init__()\n",
                "        pe = torch.zeros(max_len, d_model)\n",
                "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
                "        slope = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
                "        pe[:, 0::2] = torch.sin(position * slope); pe[:, 1::2] = torch.cos(position * slope)\n",
                "        self.register_buffer('pe', pe.unsqueeze(0))\n",
                "    def forward(self, x): return (x * np.sqrt(x.size(-1))) + self.pe[:, :x.size(1), :]\n",
                "\n",
                "class MultiHeadedAttention(nn.Module):\n",
                "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.n_heads, self.d_model, self.d_k = n_heads, d_model, d_model // n_heads\n",
                "        self.l_q, self.l_k, self.l_v, self.l_o = [nn.Linear(d_model, d_model) for _ in range(4)]; self.dropout = nn.Dropout(dropout)\n",
                "    def split_heads(self, x): return x.view(x.size(0), -1, self.n_heads, self.d_k).transpose(1, 2)\n",
                "    def init_keys(self, key): self.pk = self.split_heads(self.l_k(key)); self.pv = self.split_heads(self.l_v(key))\n",
                "    def forward(self, query, mask=None):\n",
                "        q = self.split_heads(self.l_q(query))\n",
                "        scores = torch.matmul(q, self.pk.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
                "        if mask is not None: scores = scores.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
                "        alphas = self.dropout(F.softmax(scores, dim=-1))\n",
                "        return self.l_o(torch.matmul(alphas, self.pv).transpose(1, 2).contiguous().view(query.size(0), -1, self.d_model))\n",
                "\n",
                "class SubLayerWrapper(nn.Module):\n",
                "    def __init__(self, d_model, dropout): super().__init__(); self.norm, self.drop = nn.LayerNorm(d_model), nn.Dropout(dropout)\n",
                "    def forward(self, x, sublayer, is_self_attn=False, **kwargs):\n",
                "        nx = self.norm(x)\n",
                "        if is_self_attn: sublayer.init_keys(nx)\n",
                "        return x + self.drop(sublayer(nx, **kwargs))\n",
                "\n",
                "class EncoderLayer(nn.Module):\n",
                "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
                "        super().__init__(); self.self_attn = MultiHeadedAttention(n_heads, d_model, dropout)\n",
                "        self.ffn = nn.Sequential(nn.Linear(d_model, ff_units), nn.ReLU(), nn.Dropout(dropout), nn.Linear(ff_units, d_model))\n",
                "        self.subs = nn.ModuleList([SubLayerWrapper(d_model, dropout) for _ in range(2)])\n",
                "    def forward(self, x, mask=None):\n",
                "        x = self.subs[0](x, self.self_attn, is_self_attn=True, mask=mask)\n",
                "        return self.subs[1](x, self.ffn)\n",
                "\n",
                "class DecoderLayer(nn.Module):\n",
                "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
                "        super().__init__(); self.self_attn = MultiHeadedAttention(n_heads, d_model, dropout)\n",
                "        self.cross_attn = MultiHeadedAttention(n_heads, d_model, dropout)\n",
                "        self.ffn = nn.Sequential(nn.Linear(d_model, ff_units), nn.ReLU(), nn.Dropout(dropout), nn.Linear(ff_units, d_model))\n",
                "        self.subs = nn.ModuleList([SubLayerWrapper(d_model, dropout) for _ in range(3)])\n",
                "    def init_keys(self, states): self.cross_attn.init_keys(states)\n",
                "    def forward(self, x, s_mask=None, t_mask=None):\n",
                "        x = self.subs[0](x, self.self_attn, is_self_attn=True, mask=t_mask)\n",
                "        x = self.subs[1](x, self.cross_attn, mask=s_mask)\n",
                "        return self.subs[2](x, self.ffn)\n",
                "\n",
                "class EncoderDecoderTransf(nn.Module):\n",
                "    def __init__(self, n_layers, n_heads, d_model, ff_units, vocab_size, max_len=512):\n",
                "        super().__init__(); self.pe = PositionalEncoding(max_len, d_model)\n",
                "        self.enclayers = nn.ModuleList([EncoderLayer(n_heads, d_model, ff_units) for _ in range(n_layers)])\n",
                "        self.declayers = nn.ModuleList([DecoderLayer(n_heads, d_model, ff_units) for _ in range(n_layers)])\n",
                "        self.src_embed = nn.Embedding(vocab_size, d_model); self.tgt_embed = nn.Embedding(vocab_size, d_model)\n",
                "        self.out_linear = nn.Linear(d_model, vocab_size); self.norm = nn.LayerNorm(d_model)\n",
                "        self.register_buffer('mask', (1 - torch.triu(torch.ones((1, max_len, max_len)), diagonal=1)))\n",
                "\n",
                "    def forward(self, src, tgt):\n",
                "        x = self.pe(self.src_embed(src))\n",
                "        for l in self.enclayers: x = l(x)\n",
                "        enc_states = self.norm(x)\n",
                "        for l in self.declayers: l.init_keys(enc_states)\n",
                "        y = self.pe(self.tgt_embed(tgt)); L = tgt.size(1)\n",
                "        for l in self.declayers: y = l(y, t_mask=self.mask[:, :L, :L])\n",
                "        return self.out_linear(self.norm(y))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Accelerated Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train():\n",
                "    # SAFER HYPERPARAMETERS TO AVOID OOM\n",
                "    batch_size, epochs, lr = 64, 10, 1e-4\n",
                "    d_model, n_heads, n_layers, ff_units = 256, 8, 4, 512\n",
                "\n",
                "    train_loader, val_loader, tokenizer = get_dataloaders(batch_size=batch_size)\n",
                "    model = EncoderDecoderTransf(n_layers, n_heads, d_model, ff_units, tokenizer.vocab_size).to(device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "        model.train(); total_loss = 0\n",
                "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
                "        for batch in pbar:\n",
                "            ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
                "            dec_in, targets = labels[:, :-1], labels[:, 1:].contiguous()\n",
                "            optimizer.zero_grad()\n",
                "            logits = model(ids, dec_in)\n",
                "            loss = criterion(logits.view(-1, tokenizer.vocab_size), targets.view(-1))\n",
                "            loss.backward(); optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'ppl': f\"{math.exp(loss.item()):.2f}\"})\n",
                "        \n",
                "        model.eval(); v_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for b in val_loader:\n",
                "                ids, labels = b['input_ids'].to(device), b['labels'].to(device)\n",
                "                v_loss += criterion(model(ids, labels[:, :-1]).view(-1, tokenizer.vocab_size), labels[:, 1:].contiguous().view(-1)).item()\n",
                "        \n",
                "        print(f\"Epoch {epoch+1} Val Loss: {v_loss/len(val_loader):.4f}\")\n",
                "        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pt')\n",
                "\n",
                "train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}